{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analiza danych z pliku tekstowego Cell_Phones_&_Accessories.txt\n",
    "***\n",
    "# 1 Cel zadania\n",
    "Celem w zadaniu jest analiza danych z pliku tekstowego Cell_Phones_&_Accessories.txt oraz uzyskanie z niego jak największej ilości informacji.\n",
    "# 2 Propozycje informacji które można uzyskać z tekstu\n",
    "- Ilu gwiazdkom odpowiada dana recenzja? Pozowli wyeliminować system oceniania za pomocą gwiazdek\n",
    "- Generowanie tytułu artykułu na podstawie tekstu w nim zawartego.\n",
    "- \n",
    "# Uwaga\n",
    "Komentarze we fragmentach kodu pisane są w języku angielskim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "**Drugim zadaniem będzie stworzenie sieci neuronowej która będzie w stanie na podstawie tekstu recenzji generować będzie jej krótki opis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Przed przystąpieniem do uzyskiwania informacji należy wczytać dane do programu. Utwórzmy w tym celu generator kolejnych recenzji**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense, Dropout, LSTM, concatenate\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import Input\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "phones_file_name: str\n",
    "phones_file_name = \"Cell_Phones_&_Accessories.txt\"\n",
    "# Just the name of our input file. Now we dont need to remember it \n",
    "\n",
    "def parse_data(filename: str):\n",
    "    \"\"\"This function creates a generator for the data in the document \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename: str\n",
    "        A file name we want to read from\n",
    "    \"\"\"\n",
    "    file = open(filename, 'r')\n",
    "    entry: dict\n",
    "    entry = {}\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        colonPos = line.find(':')\n",
    "        if colonPos == -1:\n",
    "            yield entry\n",
    "            entry = {}\n",
    "            continue\n",
    "        elem_name = line[:colonPos]\n",
    "        elem_val = line[colonPos+2:]\n",
    "        entry[elem_name] = elem_val\n",
    "    yield entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stworzenie funkcji która zapewni dane treningowe. W sieci neuronowej mamy 2 wejścia:\n",
    "1 Wejście tekstu recenzji\n",
    "2 Wejście aktualnego stanu tytułu\n",
    "\n",
    "Tytuł recenzji we wczytanym przez nas pliku tekstowym ma $N$ słów i ma on postać:\n",
    "$[słowo1, słowo2,...,słowoN]$.\n",
    "\n",
    "Możemy zatem wyodrębnić z niego następujące sekwencje:\n",
    "$[]$\n",
    "\n",
    "$[słowo1]$\n",
    "\n",
    "$[słowo1, słowo2]$\n",
    "\n",
    "        .\n",
    "        \n",
    "        .\n",
    "        \n",
    "        .\n",
    "        \n",
    "$[słowo1, słowo2,...,słowoN]$\n",
    "\n",
    "Dla każdej recenzji będziemy mieli zatem $N+1$ możliwych kombinacji sekwencji danych w tytule.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_n_data(gen: generator, n: int, attr_data: str,\n",
    "                attr_labels: str) -> tuple(list, list):\n",
    "    \"\"\"This function creates data and labels using generator\n",
    "    \n",
    "    This function creates data and labels using generator\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    gen: generator\n",
    "        We will generate values from this generator\n",
    "    attr_data: str\n",
    "        A dictionary key defining a value we want to append to the output list\n",
    "    attr_labels: str\n",
    "        A dictionary key defining a value we want to append to the output list\n",
    "    Returns\n",
    "    ----------\n",
    "    list\n",
    "        A list containing data we were asking for\n",
    "    list\n",
    "        A list containing labels we were asking for\n",
    "    \n",
    "    \"\"\"\n",
    "    out_data: list = []\n",
    "    out_labels: list = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        next_dict = next(gen)\n",
    "        if next_dict == {}:\n",
    "            # Out of data in generator\n",
    "            return (out_data, out_labels)\n",
    "        next_value_data: dict = next_dict.get(attr_data)\n",
    "        # Get the data from our dict\n",
    "        \n",
    "        next_value_labels = next_dict.get(attr_labels)    \n",
    "        while(next_value_data is None or next_value_labels is None):\n",
    "            # Given attributes not found. Check next\n",
    "            next_dict = next(gen)\n",
    "            if next_dict == {}:\n",
    "                # We are out of data!Just return what we have# Just return what we have\n",
    "                return (out_data, out_labels)\n",
    "                \n",
    "            next_value_data = next_dict.get(attr_data)\n",
    "            next_value_labels = next_dict.get(attr_labels)-1\n",
    "        out_data.append(next_value_data)\n",
    "        out_labels.append(next_value_labels)\n",
    "    return (out_data, out_labels)\n",
    "\n",
    "def generate_dataset_titles(gen: generator, attr_data: str, attr_labels: str,\n",
    "                            train_n: int = 230000, \n",
    "                            max_words: int = 20000) -> tuple[tuple[list, list],\n",
    "                                                  tuple[list, list],\n",
    "                                                  tuple[list, list]]:   \n",
    "    \"\"\"This function creates a dataset for title creation by using a generator\n",
    "    \n",
    "    This function creates a dataset for title creation by using a generator\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    gen: generator\n",
    "        We will generate values from this generator\n",
    "    attr_data: str\n",
    "        A dictionary key defining a data we want to append to the output list\n",
    "    attr_labels: str\n",
    "        A dictionary key defining a labels we want to append to the output list\n",
    "    train_n: int\n",
    "        Defines a size of the data lists\n",
    "    max_words: int\n",
    "        Number of unique words in dataset\n",
    "    Returns\n",
    "    ----------\n",
    "    tokenizer:\n",
    "        A tokenizer used to tokenize reviews\n",
    "    tuple[list, list, list]\n",
    "        A tuple of train_review_data, train_title_data, train_labels\n",
    "    tuple[list, list, list]\n",
    "        A tuple of test_review_data, test_title_data, test_labels\n",
    "\n",
    "    \"\"\"\n",
    "    # Train data and labels\n",
    "    str_train_review_data: list = []\n",
    "    str_train_title_data: list = []    \n",
    "    str_train_labels: list = []\n",
    "\n",
    "    out_train_review_data: list = []\n",
    "    out_train_title_data: list = []   \n",
    "    out_train_labels: list = []    \n",
    "    \n",
    "    # Test data and labels\n",
    "    str_test_review_data: list = []\n",
    "    str_test_title_data: list = []\n",
    "    str_test_title_labels: list = []\n",
    "\n",
    "    out_test_review_data: list = []\n",
    "    out_test_title_data: list = []\n",
    "    out_test_labels: list = []\n",
    "    \n",
    "    str_all_data: list = []\n",
    "    str_all_labels: list = []\n",
    "\n",
    "    out_all_review_data: list = []\n",
    "    out_all_title_data: list = []\n",
    "    out_all_labels: list = [] \n",
    "    \n",
    "    # Load all possible data\n",
    "    (str_all_data, str_all_labels) = load_n_data(gen, 100000, attr_data, attr_labels)\n",
    "    \n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    # Fit using all the data\n",
    "    tokenizer.fit_on_texts(str_all_data)\n",
    "    temp_all_data = tokenizer.texts_to_sequences(str_all_data)\n",
    "    temp_all_labels = tokenizer.texts_to_sequences(str_all_labels)\n",
    "    \n",
    "    # Split data for different inputs and generate correnc output\n",
    "    for i, review in enumerate(temp_all_data):\n",
    "        for j, word in enumerate(temp_all_labels[i]):\n",
    "            out_all_labels.append(word)\n",
    "            out_all_review_data.append(review)\n",
    "            \n",
    "            if j == len(temp_all_labels[i])-1:\n",
    "                break\n",
    "            elif j == 0:\n",
    "                out_all_title_data.append([])\n",
    "            else:\n",
    "                new_item: list = out_all_title_data[-1] + [temp_all_labels[i][j-1]]\n",
    "                out_all_title_data.append(new_item)\n",
    "            \n",
    "    # Just to randomize set\n",
    "    list_all = list(zip(out_all_review_data, out_all_title_data, out_all_labels))\n",
    "    random.shuffle(list_all)  \n",
    "    out_all_review_data, out_all_title_data, out_all_labels = zip(*list_all)\n",
    "    \n",
    "    \n",
    "    # Generate train raw data\n",
    "    out_train_review_data = out_all_review_data[:train_n]\n",
    "    out_train_title_data = out_all_title_data[:train_n]\n",
    "    out_train_labels = out_all_labels[:train_n]\n",
    "    # Generate test raw data\n",
    "    out_test_review_data = out_all_review_data[train_n:]\n",
    "    out_test_title_data = out_all_title_data[train_n:] \n",
    "    out_test_labels = out_all_labels[train_n:] \n",
    "    # Generate validation data. It contains the rest of data\n",
    "    #out_val_data = out_all_data[train_n+test_n:]\n",
    "    #out_val_labels = out_all_labels[train_n+test_n:]\n",
    "        \n",
    "    return tokenizer, (out_train_review_data, out_train_title_data, out_train_labels), (out_test_review_data, out_test_title_data, out_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przy użyciu funkcji generującej utwórzmy dataset. W zbiorze treningowym znajdzie się 230000 próbek, a w testowym reszta, czyli 24.446"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230000\n",
      "230000\n",
      "230000\n",
      "24446\n",
      "24446\n",
      "24446\n"
     ]
    }
   ],
   "source": [
    "max_words = 20000\n",
    "max_len_review = 500\n",
    "max_len_title = 10\n",
    "gen = parse_data(phones_file_name)\n",
    "my_tokenizer, (train_review_data, train_title_data, train_labels), (test_review_data, test_title_data, test_labels) = generate_dataset_titles(gen, \"review/text\", \"review/summary\", max_words=max_words)\n",
    "\n",
    "# Just to be sure\n",
    "print(len(train_review_data))\n",
    "print(len(train_title_data))\n",
    "print(len(train_labels))\n",
    "print(len(test_review_data))\n",
    "print(len(test_title_data))\n",
    "print(len(test_labels))\n",
    "\n",
    "train_review_data = pad_sequences(train_review_data, maxlen = max_len_review)\n",
    "test_review_data = pad_sequences(test_review_data, maxlen = max_len_review)\n",
    "\n",
    "train_title_data = pad_sequences(train_title_data, maxlen = max_len_title)\n",
    "test_title_data = pad_sequences(test_title_data, maxlen = max_len_title)\n",
    "\n",
    "train_labels = to_categorical(train_labels, num_classes=max_words)\n",
    "test_labels = to_categorical(test_labels, num_classes=max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stwórzmy funkcję generującą następne słowo na podstawie rozkładu prawdopodobieństwa wystąpienia go. Większy epsilon to większa losowość działań. Dla 1.0 korzystamy z rozkładu podanego na wejściu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word(predictions:np.array, epsilon:float=0.5):\n",
    "    predictions = np.log(predictions) / epsilon\n",
    "    predictions = np.exp(predictions)\n",
    "    predictions = predictions/ np.sum(predictions)\n",
    "    pred: float = np.random.multinomial(1, predictions, 1)\n",
    "    return np.argmax(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Próbujemy przewidzieć następne słowa na podstawie dwóch wejść.\n",
    "\n",
    "1 Tekst recenzji <br>\n",
    "2 Słowa które znalazły się już w tytule recenzji i czekają na kompana\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LSTM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-1044b1653b34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0membedded_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_len_title\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mlstm_review\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lstm_review'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded_review\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mlstm_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lstm_labels'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LSTM' is not defined"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    shutil.rmtree('results/generator')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "os.mkdir('results/generator')\n",
    "\n",
    "callbacks = [TensorBoard(log_dir='results/conv', histogram_freq=1, embeddings_freq=1)]\n",
    "\n",
    "input_review = Input(shape=(None,), name='review')\n",
    "input_labels = Input(shape=(None,), name='title')\n",
    "\n",
    "embedded_review = Embedding(max_words, 128, input_length=max_len_review)(input_review)\n",
    "embedded_labels = Embedding(max_words, 128, input_length=max_len_title)(input_labels)\n",
    "\n",
    "lstm_review = LSTM(64, name='lstm_review', dropout=0.2)(embedded_review)\n",
    "lstm_labels = LSTM(32, name='lstm_labels', dropout=0.2)(embedded_labels)\n",
    "\n",
    "concat = concatenate([lstm_review, lstm_labels], axis=-1, name='concat')\n",
    "ans = Dense(max_words, activation='softmax', name='out')(concat)\n",
    "\n",
    "model_generator = Model([input_review, input_labels], ans)\n",
    "\n",
    "model_generator.compile(optimizer=RMSprop(lr=0.001), loss='categorical_crossentropy', metrics='accuracy')\n",
    "\n",
    "plot_model(model_generator, to_file='model_generator.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_generator = model_generator.fit({'review': train_review_data,\n",
    "                                         'title': train_title_data},\n",
    "                                        train_labels,\n",
    "                                        validation_split=0.1,\n",
    "                                        epochs=1,\n",
    "                                        batch_size=128)\n",
    "\n",
    "print(train_review_data.shape)\n",
    "print(model_generator.evaluate({'review': test_review_data, 'title': test_title_data}, test_labels))\n",
    "                            \n",
    "acc = history_generator.history['accuracy']\n",
    "val_acc = history_generator.history['val_accuracy']\n",
    "\n",
    "\n",
    "plt.plot(range(1, len(acc)+1), acc, 'rx', label='Train acc')\n",
    "plt.plot(range(1, len(acc)+1), val_acc, 'b', label='Val acc')\n",
    "plt.title('Train & val accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()                 \n",
    "model_simple.save_weights(\"conv.h5\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
