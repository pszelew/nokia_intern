{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analiza danych z pliku tekstowego Cell_Phones_&_Accessories.txt\n",
    "***\n",
    "# 1 Cel zadania\n",
    "Celem w zadaniu jest analiza danych z pliku tekstowego Cell_Phones_&_Accessories.txt oraz uzyskanie z niego jak największej ilości informacji.\n",
    "# 2 Propozycje informacji które można uzyskać z tekstu\n",
    "- Ilu gwiazdkom odpowiada dana recenzja? Pozowli wyeliminować system oceniania za pomocą gwiazdek\n",
    "- W jakim stopniu ocena ta jest użyteczna, czy wyświetlać ją pierwszą w zestawieniu ocen?\n",
    "- \n",
    "# Uwaga\n",
    "Komentarze we fragmentach kodu pisane są w języku angielskim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Przed przystąpieniem do uzyskiwania informacji należy wczytać dane do programu. Utwórzmy w tym celu generator kolejnych recenzji**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "# It helps us to create annotations\n",
    "phones_file_name: str\n",
    "phones_file_name = \"Cell_Phones_&_Accessories.txt\"\n",
    "# Just the name of our input file. Now we dont need to remember it \n",
    "\n",
    "def parse_data(filename: str):\n",
    "    \"\"\"This function creates a generator for the data in the document \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename: str\n",
    "        A file name we want to read from\n",
    "    \"\"\"\n",
    "    file = open(filename, 'r')\n",
    "    entry: dict\n",
    "    entry = {}\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        colonPos = line.find(':')\n",
    "        if colonPos == -1:\n",
    "            yield entry\n",
    "            entry = {}\n",
    "            continue\n",
    "        elem_name = line[:colonPos]\n",
    "        elem_val = line[colonPos+2:]\n",
    "        entry[elem_name] = elem_val\n",
    "    yield entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utwórzmy dataset z naszego generatora. Tekst przetworzymy na zbiór liczb całkowitych za pomocą słownika. Każde słowo to inna liczba całkowita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import random\n",
    "\n",
    "\n",
    "def load_n_data(gen: generator, n: int, attr_data: str,\n",
    "                attr_labels: str) -> tuple(list, list):\n",
    "    \"\"\"This function creates data and labels using generator\n",
    "    \n",
    "    This function creates data and labels using generator\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    gen: generator\n",
    "        We will generate values from this generator\n",
    "    attr_data: str\n",
    "        A dictionary key defining a value we want to append to the output list\n",
    "    attr_labels: str\n",
    "        A dictionary key defining a value we want to append to the output list\n",
    "    Returns\n",
    "    ----------\n",
    "    list\n",
    "        A list containing data we were asking for\n",
    "    list\n",
    "        A list containing labels we were asking for\n",
    "    \n",
    "    \"\"\"\n",
    "    out_data: list = []\n",
    "    out_labels: list = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        next_dict = next(gen)\n",
    "        if next_dict == {}:\n",
    "            # Out of data in generator\n",
    "            return (out_data, out_labels)\n",
    "        next_value_data: dict = next_dict.get(attr_data)\n",
    "        # Get the data from our dict\n",
    "        \n",
    "        next_value_labels = next_dict.get(attr_labels)    \n",
    "        while(next_value_data is None or next_value_labels is None):\n",
    "            # Given attributes not found. Check next\n",
    "            next_dict = next(gen)\n",
    "            if next_dict == {}:\n",
    "                # We are out of data!Just return what we have# Just return what we have\n",
    "                return (out_data, out_labels)\n",
    "                \n",
    "            next_value_data = next_dict.get(attr_data)\n",
    "            next_value_labels = next_dict.get(attr_labels)-1\n",
    "        out_data.append(next_value_data)\n",
    "        out_labels.append(next_value_labels)\n",
    "    return (out_data, out_labels)\n",
    "\n",
    "def generate_dataset(gen: generator, attr_data: str, attr_labels: str,train_n: int = 64000,\n",
    "                     test_n: int = 8000, max_words: int = 20000) -> tuple[tuple[list, list],\n",
    "                                                  tuple[list, list],\n",
    "                                                  tuple[list, list]]:   \n",
    "    \"\"\"This function creates a dataset by using a generator\n",
    "    \n",
    "    This function creates a dataset by using a generator\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    gen: generator\n",
    "        We will generate values from this generator\n",
    "    attr_data: str\n",
    "        A dictionary key defining a data we want to append to the output list\n",
    "    attr_labels: str\n",
    "        A dictionary key defining a labels we want to append to the output list\n",
    "    train_n: int\n",
    "        Defines a size of the data lists\n",
    "    test_n: int\n",
    "        Defines a size of the test lists\n",
    "    max_words: int\n",
    "        Number of unique words in dataset\n",
    "    Returns\n",
    "    ----------\n",
    "    tokenizer:\n",
    "        Used tokenizer\n",
    "    tuple[list, list]\n",
    "        A tuple of train_data and train_labels\n",
    "    tuple[list, list]\n",
    "        A tuple of test_data and test_labels\n",
    "    tuple[list, list]\n",
    "        A tuple of val_data and val_labels. It uses the rest of data from generator\n",
    "        (total_n-train_n-test_m)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Train data and labels\n",
    "    str_train_data: list = []\n",
    "    str_train_labels: list = []\n",
    "    out_train_data: list = []\n",
    "    out_train_labels: list = []\n",
    "    \n",
    "    # Test data and labels\n",
    "    str_test_data: list = []\n",
    "    str_test_labels: list = []\n",
    "    out_test_data: list = []\n",
    "    out_test_labels: list = []\n",
    "    \n",
    "    # Validate data and labels\n",
    "    str_val_data: list = []\n",
    "    str_val_labels: list = []\n",
    "    out_val_data: list = []\n",
    "    out_val_labels: list = []\n",
    "    \n",
    "    str_all_data: list = []\n",
    "    str_all_labels: list = []\n",
    "    out_all_data: list = []\n",
    "    # out_all_labels: list = []\n",
    "    \n",
    "    # Load all possible data\n",
    "    (str_all_data, str_all_labels) = load_n_data(gen, 100000, attr_data, attr_labels)\n",
    "    \n",
    "    # Just to randomize set\n",
    "    list_all = list(zip(str_all_data, str_all_labels))\n",
    "    random.shuffle(list_all)\n",
    "    \n",
    "    str_all_data, str_all_labels = zip(*list_all)\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    # Fit using all the data\n",
    "    tokenizer.fit_on_texts(str_all_data)\n",
    "    out_all_data = tokenizer.texts_to_sequences(str_all_data)\n",
    "    \n",
    "    \n",
    "     # Generate train raw data\n",
    "    out_train_data = out_all_data[:train_n]\n",
    "    str_train_labels = str_all_labels[:train_n]\n",
    "    # Generate test raw data\n",
    "    out_test_data = out_all_data[train_n:train_n+test_n] \n",
    "    str_test_labels = str_all_labels[train_n:train_n+test_n] \n",
    "    # Generate validation data. It contains the rest of data\n",
    "    out_val_data = out_all_data[train_n+test_n:]\n",
    "    str_val_labels = str_all_labels[train_n+test_n:]\n",
    "    \n",
    "    # Cast to int and substract 1 to get range (0-1)\n",
    "    out_train_labels = [(int(x.replace(\".0\", \"\"))-1)/4 for x in str_train_labels]\n",
    "    out_test_labels = [(int(x.replace(\".0\", \"\"))-1)/4 for x in str_test_labels]\n",
    "    out_val_labels = [(int(x.replace(\".0\", \"\"))-1)/4 for x in str_val_labels]\n",
    "    \n",
    "    \n",
    "    return tokenizer, (out_train_data, out_train_labels), (out_test_data, out_test_labels), (out_val_data, out_val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pierwszym zadaniem będzie stworzenie sieci neuronowej która będzie w stanie ocenić czy recenzja jest pozytywna, czy negatywna**\n",
    "\n",
    "Zakładam, że punktowa skala ocen w danych wejściowych (1-5) jest miarodajnym źródłem informacji na temat tego, czy dana ocena produktu jest pozytywna. Potraktuję zatem pola \"review/score\" jako źródło 'etykiet' (ang. *labels*) dla zaporoponowanej przeze mnie sieci neuronowej. Wejściem sieci będzie natomiast tekst recenzji. Sieć generować będzie proponowaną ocenę na podstawie tekstu recenzji. Sklep może sugerować ocenę produktu, co może pomóc niezdecydowanym recenzującym.\n",
    "Jest to zadanie regresji wartości z zakresu od 0 do 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdźmy na początek ile danych w pliku mamy do dyspozycji. Będzie to przydatne przy dzieleniu recenzji na zbiory: treningowy. testowy oraz walidacyjny. W sumie to dość oczywista rzecz, ale trzeba to sprawdzić. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78930\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for data in parse_data(phones_file_name):\n",
    "    if data != {}:\n",
    "        i+=1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak widać mamy 78930 unikalnych wpisów. Załóżmy że około 80% danych to zbiór treningowy, 10% zbiór treningowy, 10% zbiór walidacyjny. Zatem będziemy mieli 64000 recenzji treningowych, 8000 testowych, a reszta to zbiór walidacyjny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64000\n",
      "64000\n",
      "8000\n",
      "8000\n",
      "6930\n",
      "6930\n"
     ]
    }
   ],
   "source": [
    "max_words = 20000\n",
    "gen = parse_data(phones_file_name)\n",
    "my_tokenizer, (train_data, train_labels), (test_data, test_labels), (val_data, val_labels) = generate_dataset(gen, \"review/text\", \"review/score\", max_words=max_words)\n",
    "# Just to be sure\n",
    "print(len(train_data))\n",
    "print(len(train_labels))\n",
    "print(len(test_data))\n",
    "print(len(test_labels))\n",
    "print(len(val_data))\n",
    "print(len(val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skróćmy jeszcze nasze \"teksty\" recenzji do 500 liczb, bądź wypełnijmy je wartościami 0, tak by każda z nich była wektorem o równej długości."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "max_len = 500\n",
    "\n",
    "train_data = pad_sequences(train_data, maxlen = max_len)\n",
    "test_data = pad_sequences(test_data, maxlen = max_len)\n",
    "val_data = pad_sequences(val_data, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Niech etykiety będą wektorami numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)\n",
    "val_labels = np.array(val_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zadanie zacznijmy od napisania \"algorytmu\" który sprawdzi linię bazowa dla problemu. Skoro wartości znormalizowanych ocen mogą przyjąć wartości $0$, $0.25$, $0.5$, $0.75$, $1.0$, to najmniejszy błąd predykcji, wybierając średnią z danych testowych. Policzmy ją i wstawmy ją do prostej funkcji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33845860937500005\n"
     ]
    }
   ],
   "source": [
    "baseline = np.mean(train_labels)\n",
    "\n",
    "def my_mae(test_labels: np.array) -> float:\n",
    "    maes:list = []\n",
    "    mae:float\n",
    "    for label in test_labels:\n",
    "        mae = np.mean(np.abs(baseline - label))\n",
    "        maes.append(mae)\n",
    "    return np.mean(maes)\n",
    "\n",
    "print(my_mae(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otrzymujemy informację, że ślepo podążając takim naiwnym rozwiązaniem otrzymujemy średnio błąd na poziomie $0.33$. Wszystko co da błąd mniejszy niż ta wartość można uznać że jest już w jakimś stopniu inteligentne :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dane są teraz w idealnej postaci do przekazania ich do warstwy osadzającej *embedding*. Dzięki niej modelować będziemy mogli modelować subtelne zależności między słowami.\n",
    "\n",
    "Stwórzmy prostą sieć która postara się wykonać zadanie regresji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "# We will use tensorboard to monitor training. It will be helpfull\n",
    "\n",
    "try:\n",
    "    shutil.rmtree('results/simple')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "os.mkdir('results/simple')\n",
    "\n",
    "callbacks = [TensorBoard(log_dir='results/simple', histogram_freq=1, embeddings_freq=1)]\n",
    "\n",
    "# Create sequential model of network. It's just simple model to check if it works somehow\n",
    "# model_simple = Sequential(name=\"simple\")\n",
    "# model_simple.add(Embedding(max_words, 128, input_length=max_len)) # n x max_len x 512 \n",
    "# model_simple.add(Flatten()) # n x max_len*512\n",
    "# model_simple.add(Dense(32, activation='relu'))\n",
    "# model_simple.add(Dropout(0.5))\n",
    "# model_simple.add(Dense(1, activation='sigmoid'))\n",
    "# model_simple.compile(optimizer=RMSprop(learning_rate=0.001), loss='mse', metrics=[\"mae\"])\n",
    "# init_weights = model.get_weights()\n",
    "\n",
    "\n",
    "# history = model_simple.fit(train_data, train_labels,\n",
    "#                            epochs=5, batch_size=32,\n",
    "#                            validation_data=(val_data, val_labels),\n",
    "#                            callbacks = callbacks)\n",
    "# # model.fit(np.concatenate((train_data, val_data), axis=0), np.concatenate((train_labels,val_labels), axis=0), epochs=4, batch_size=64)\n",
    "# print(model_simple.evaluate(test_data, test_labels))\n",
    "\n",
    "# acc = history.history['mae']\n",
    "# val_acc = history.history['val_mae']\n",
    "\n",
    "# plt.plot(range(1, len(acc)+1), acc, 'rx', label='Train mae')\n",
    "# plt.plot(range(1, len(acc)+1), val_acc, 'b', label='Val mae')\n",
    "# plt.title('Train & val error [mae]')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('mae')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# model_simple.save_weights(\"simple.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stwórzmy teraz prostą sieć rekurencyjną z warstwa LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "try:\n",
    "    shutil.rmtree('results/lstm')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "os.mkdir('results/lstm')\n",
    "\n",
    "\n",
    "callbacks = [TensorBoard(log_dir='results/lstm', histogram_freq=1, embeddings_freq=1)]\n",
    "\n",
    "# model_LSTM = Sequential(name=\"lstm\")\n",
    "# model_LSTM.add(Embedding(max_words, 128, input_length=max_len))\n",
    "# model_LSTM.add(LSTM(64, dropout=0.2))\n",
    "# model_LSTM.add(Dense(1, activation='sigmoid'))\n",
    "# model_LSTM.compile(optimizer=RMSprop(learning_rate=0.001), loss='mse', metrics=[\"mae\"])\n",
    "\n",
    "# history_LSTM = model_LSTM.fit(train_data, train_labels,\n",
    "#                               epochs=10, batch_size=32,\n",
    "#                               validation_data=(val_data, val_labels),\n",
    "#                               callbacks = callbacks)\n",
    "# print(model_LSTM.evaluate(test_data, test_labels))\n",
    "\n",
    "# acc = history_LSTM.history['mae']\n",
    "# val_acc = history_LSTM.history['val_mae']\n",
    "\n",
    "\n",
    "# plt.plot(range(1, len(acc)+1), acc, 'rx', label='Train mae')\n",
    "# plt.plot(range(1, len(acc)+1), val_acc, 'b', label='Val mae')\n",
    "# plt.title('Train & val error [mae]')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('mae')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# model_simple.save_weights(\"lstm.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zobaczmy jak działa dla naszego przykładu dwukierunkowa sieć rekurencyjna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "try:\n",
    "    shutil.rmtree('results/lstm2')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "os.mkdir('results/lstm2')\n",
    "\n",
    "callbacks = [TensorBoard(log_dir='results/lstm2', histogram_freq=1, embeddings_freq=1)]\n",
    "\n",
    "# model_LSTM2 = Sequential(name=\"lstm2\")\n",
    "# model_LSTM2.add(Embedding(max_words, 128, input_length=max_len))\n",
    "# model_LSTM2.add(Bidirectional(LSTM(64, dropout=0.2)))\n",
    "# model_LSTM2.add(Dense(1, activation='sigmoid'))\n",
    "# model_LSTM2.compile(optimizer=RMSprop(learning_rate=0.001), loss='mse', metrics=[\"mae\"])\n",
    "\n",
    "# history_LSTM2 = model_LSTM2.fit(train_data, train_labels,\n",
    "#                                 epochs=10, batch_size=32,\n",
    "#                                 validation_data=(val_data, val_labels),\n",
    "#                                 callbacks = callbacks)\n",
    "\n",
    "# print(model_LSTM2.evaluate(test_data, test_labels))\n",
    "                            \n",
    "# acc = history_LSTM2.history['mae']\n",
    "# val_acc = history_LSTM2.history['val_mae']\n",
    "\n",
    "\n",
    "# plt.plot(range(1, len(acc)+1), acc, 'rx', label='Train mae')\n",
    "# plt.plot(range(1, len(acc)+1), val_acc, 'b', label='Val mae')\n",
    "# plt.title('Train & val error [mae]')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "# plt.show()                 \n",
    "                   \n",
    "# model_simple.save_weights(\"lstm2.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Może jednowymiarowa sieć konwolucyjna da radę dokonać lepszej predykcji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPool1D\n",
    "\n",
    "try:\n",
    "    shutil.rmtree('results/conv')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "os.mkdir('results/conv')\n",
    "\n",
    "callbacks = [TensorBoard(log_dir='results/conv', histogram_freq=1, embeddings_freq=1)]\n",
    "\n",
    "\n",
    "# model_CONV = Sequential(name=\"conv\")\n",
    "# model_CONV.add(Embedding(max_words, 128, input_length=max_len))\n",
    "# model_CONV.add(Conv1D(32, 5, activation='relu'))\n",
    "# model_CONV.add(MaxPooling1D(5))\n",
    "# model_CONV.add(Conv1D(32, 5, activation='relu'))\n",
    "# model_CONV.add(GlobalMaxPool1D())\n",
    "# model_CONV.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# model_CONV.compile(optimizer=RMSprop(learning_rate=0.001), loss='mse', metrics=[\"mae\"])\n",
    "# history_CONV = model_CONV.fit(train_data, train_labels,\n",
    "#                                epochs=7, batch_size=32,\n",
    "#                                validation_data=(val_data, val_labels),\n",
    "#                                callbacks = callbacks)\n",
    "\n",
    "# print(model_CONV.evaluate(test_data, test_labels))\n",
    "                            \n",
    "# acc = history_CONV.history['mae']\n",
    "# val_acc = history_CONV.history['val_mae']\n",
    "\n",
    "\n",
    "# plt.plot(range(1, len(acc)+1), acc, 'rx', label='Train mae')\n",
    "# plt.plot(range(1, len(acc)+1), val_acc, 'b', label='Val mae')\n",
    "# plt.title('Train & val error [mae]')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "# plt.show()                 \n",
    "# model_simple.save_weights(\"conv.h5\")               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Najlepsze rezulaty zdaje się oferować jedna z sieci reukrurencyjnych. Sprawdźmy ocenę modelu dla napisanej przeze mnie przykładowej recenzji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_review_negative:list = [\"This phone is bad\"]\n",
    "# my_review_positive:list = [\"it is an excelent good phone\"]\n",
    "\n",
    "# # Just some preparations\n",
    "# review_data_negative = my_tokenizer.texts_to_sequences(my_review_negative)\n",
    "# review_data_negative = pad_sequences(review_data_negative, maxlen = max_len)\n",
    "\n",
    "# review_data_positive = my_tokenizer.texts_to_sequences(my_review_positive)\n",
    "# review_data_positive = pad_sequences(review_data_positive, maxlen = max_len)\n",
    "\n",
    "# print(model_LSTM.predict(review_data_negative)*4+1)\n",
    "# print(model_LSTM.predict(review_data_positive)*4+1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak widać użycie słów negatywnych działa wybornie w tym przykładzie. Algorytm ocenił go na około (0.17*4+1 = 1.68 gwiazdki). Użycie słów polecających również działa tak jak ma działać ocena to około (0.94*4+1 = 4.77 gwiazdki)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W sumie to było dość proste zadanie regresji. Muszę całość działa całkiem ok. Teraz coś trudniejszego\n",
    "\n",
    "**Drugim zadaniem będzie stworzenie sieci neuronowej która będzie w stanie na podstawie tekstu recenzji generować będzie jej krótki opis**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stworzenie funkcji która zapewni date treningowe. W sieci neuronowej mamy 2 wejścia:\n",
    "1 Wejście tekstu recenzji\n",
    "2 Wejście aktualnego stanu tytułu\n",
    "\n",
    "Tytuł recenzji we wczytanym przez nas pliku tekstowym ma $N$ słów i ma on postać:\n",
    "$[słowo1, słowo2,...,słowoN]$.\n",
    "\n",
    "Możemy zatem wyodrębnić z niego następujące sekwencje:\n",
    "$[]$\n",
    "\n",
    "$[słowo1]$\n",
    "\n",
    "$[słowo1, słowo2]$\n",
    "\n",
    "        .\n",
    "        \n",
    "        .\n",
    "        \n",
    "        .\n",
    "        \n",
    "$[słowo1, słowo2,...,słowoN]$\n",
    "\n",
    "Dla każdej recenzji będziemy mieli zatem $N+1$ możliwych kombinacji sekwencji danych w tytule.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_titles(gen: generator, attr_data: str, attr_labels: str,\n",
    "                            train_n: int = 230000, \n",
    "                            max_words: int = 20000) -> tuple[tuple[list, list],\n",
    "                                                  tuple[list, list],\n",
    "                                                  tuple[list, list]]:   \n",
    "    \"\"\"This function creates a dataset for title creation by using a generator\n",
    "    \n",
    "    This function creates a dataset for title creation by using a generator\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    gen: generator\n",
    "        We will generate values from this generator\n",
    "    attr_data: str\n",
    "        A dictionary key defining a data we want to append to the output list\n",
    "    attr_labels: str\n",
    "        A dictionary key defining a labels we want to append to the output list\n",
    "    train_n: int\n",
    "        Defines a size of the data lists\n",
    "    max_words: int\n",
    "        Number of unique words in dataset\n",
    "    Returns\n",
    "    ----------\n",
    "    tokenizer:\n",
    "        A tokenizer used to tokenize reviews\n",
    "    tuple[list, list, list]\n",
    "        A tuple of train_review_data, train_title_data, train_labels\n",
    "    tuple[list, list, list]\n",
    "        A tuple of test_review_data, test_title_data, test_labels\n",
    "\n",
    "    \"\"\"\n",
    "    # Train data and labels\n",
    "    str_train_review_data: list = []\n",
    "    str_train_title_data: list = []    \n",
    "    str_train_labels: list = []\n",
    "\n",
    "    out_train_review_data: list = []\n",
    "    out_train_title_data: list = []   \n",
    "    out_train_labels: list = []    \n",
    "    \n",
    "    # Test data and labels\n",
    "    str_test_review_data: list = []\n",
    "    str_test_title_data: list = []\n",
    "    str_test_title_labels: list = []\n",
    "\n",
    "    out_test_review_data: list = []\n",
    "    out_test_title_data: list = []\n",
    "    out_test_labels: list = []\n",
    "    \n",
    "    str_all_data: list = []\n",
    "    str_all_labels: list = []\n",
    "\n",
    "    out_all_review_data: list = []\n",
    "    out_all_title_data: list = []\n",
    "    out_all_labels: list = [] \n",
    "    \n",
    "    # Load all possible data\n",
    "    (str_all_data, str_all_labels) = load_n_data(gen, 100000, attr_data, attr_labels)\n",
    "    \n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    # Fit using all the data\n",
    "    tokenizer.fit_on_texts(str_all_data)\n",
    "    temp_all_data = tokenizer.texts_to_sequences(str_all_data)\n",
    "    temp_all_labels = tokenizer.texts_to_sequences(str_all_labels)\n",
    "    \n",
    "    # Split data for different inputs and generate correnc output\n",
    "    for i, review in enumerate(temp_all_data):\n",
    "        for j, word in enumerate(temp_all_labels[i]):\n",
    "            out_all_labels.append(word)\n",
    "            out_all_review_data.append(review)\n",
    "            \n",
    "            if j == len(temp_all_labels[i])-1:\n",
    "                break\n",
    "            elif j == 0:\n",
    "                out_all_title_data.append([])\n",
    "            else:\n",
    "                new_item: list = out_all_title_data[-1] + [temp_all_labels[i][j-1]]\n",
    "                out_all_title_data.append(new_item)\n",
    "            \n",
    "    # Just to randomize set\n",
    "    list_all = list(zip(out_all_review_data, out_all_title_data, out_all_labels))\n",
    "    random.shuffle(list_all)  \n",
    "    out_all_review_data, out_all_title_data, out_all_labels = zip(*list_all)\n",
    "    \n",
    "    \n",
    "    # Generate train raw data\n",
    "    out_train_review_data = out_all_review_data[:train_n]\n",
    "    out_train_title_data = out_all_title_data[:train_n]\n",
    "    out_train_labels = out_all_labels[:train_n]\n",
    "    # Generate test raw data\n",
    "    out_test_review_data = out_all_review_data[train_n:]\n",
    "    out_test_title_data = out_all_title_data[train_n:] \n",
    "    out_test_labels = out_all_labels[train_n:] \n",
    "    # Generate validation data. It contains the rest of data\n",
    "    #out_val_data = out_all_data[train_n+test_n:]\n",
    "    #out_val_labels = out_all_labels[train_n+test_n:]\n",
    "        \n",
    "    return tokenizer, (out_train_review_data, out_train_title_data, out_train_labels), (out_test_review_data, out_test_title_data, out_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przy użyciu funkcji generującej utwórzmy dataset. W zbiorze treningowym znajdzie się 230000 próbek, a w testowym reszta, czyli 24.446"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230000\n",
      "230000\n",
      "230000\n",
      "24446\n",
      "24446\n",
      "24446\n"
     ]
    }
   ],
   "source": [
    "max_words = 20000\n",
    "max_len_review = 500\n",
    "max_len_title = 10\n",
    "gen = parse_data(phones_file_name)\n",
    "my_tokenizer, (train_review_data, train_title_data, train_labels), (test_review_data, test_title_data, test_labels) = generate_dataset_titles(gen, \"review/text\", \"review/summary\", max_words=max_words)\n",
    "\n",
    "# Just to be sure\n",
    "print(len(train_review_data))\n",
    "print(len(train_title_data))\n",
    "print(len(train_labels))\n",
    "print(len(test_review_data))\n",
    "print(len(test_title_data))\n",
    "print(len(test_labels))\n",
    "\n",
    "train_review_data = pad_sequences(train_review_data, maxlen = max_len_review)\n",
    "test_review_data = pad_sequences(test_review_data, maxlen = max_len_review)\n",
    "\n",
    "train_title_data = pad_sequences(train_title_data, maxlen = max_len_title)\n",
    "test_title_data = pad_sequences(test_title_data, maxlen = max_len_title)\n",
    "\n",
    "train_labels = to_categorical(train_labels, num_classes=max_words)\n",
    "test_labels = to_categorical(test_labels, num_classes=max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stwórzmy funkcję generującą następne słowo na podstawie rozkładu prawdopodobieństwa wystąpienia go. Większy epsilon to większa losowość działań. Dla 1.0 korzystamy z rozkładu podanego na wejściu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word(predictions:np.array, epsilon:float=0.5):\n",
    "    predictions = np.log(predictions) / epsilon\n",
    "    predictions = np.exp(predictions)\n",
    "    predictions = predictions/ np.sum(predictions)\n",
    "    pred: float = np.random.multinomial(1, predictions, 1)\n",
    "    return np.argmax(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Próbujemy przewidzieć następne słowa na podstawie dwóch wejść.\n",
    "\n",
    "1 Tekst recenzji\n",
    "2 Słowa które znalazły się już w tytule recenzji i czekają na kompana\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "\n",
    "try:\n",
    "    shutil.rmtree('results/generator')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "os.mkdir('results/generator')\n",
    "\n",
    "callbacks = [TensorBoard(log_dir='results/conv', histogram_freq=1, embeddings_freq=1)]\n",
    "\n",
    "input_review = Input(shape=(None,), name='review')\n",
    "input_labels = Input(shape=(None,), name='title')\n",
    "\n",
    "embedded_review = Embedding(max_words, 128, input_length=max_len_review)(input_review)\n",
    "embedded_labels = Embedding(max_words, 128, input_length=max_len_title)(input_labels)\n",
    "\n",
    "lstm_review = LSTM(64, name='lstm_review', dropout=0.2)(embedded_review)\n",
    "lstm_labels = LSTM(32, name='lstm_labels', dropout=0.2)(embedded_labels)\n",
    "\n",
    "concat = concatenate([lstm_review, lstm_labels], axis=-1, name='concat')\n",
    "ans = Dense(max_words, activation='softmax', name='out')(concat)\n",
    "\n",
    "model_generator = Model([input_review, input_labels], ans)\n",
    "\n",
    "model_generator.compile(optimizer=RMSprop(lr=0.001), loss='categorical_crossentropy', metrics='accuracy')\n",
    "\n",
    "plot_model(model_generator, to_file='model_generator.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(230000, 500)\n",
      "764/764 [==============================] - 9s 12ms/step - loss: 9.9032 - accuracy: 1.2272e-04\n",
      "[9.903156280517578, 0.00012271945888642222]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'history_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-0df8be41df26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'review'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_review_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'title'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_title_data\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history_generator' is not defined"
     ]
    }
   ],
   "source": [
    "history_generator = model_generator.fit({'review': train_review_data,\n",
    "                                         'title': train_title_data},\n",
    "                                        train_labels,\n",
    "                                        validation_split=0.1,\n",
    "                                        epochs=20,\n",
    "                                        batch_size=128)\n",
    "\n",
    "print(train_review_data.shape)\n",
    "print(model_generator.evaluate({'review': test_review_data, 'title': test_title_data}, test_labels))\n",
    "                            \n",
    "acc = history_generator.history['accuracy']\n",
    "val_acc = history_generator.history['val_accuracy']\n",
    "\n",
    "\n",
    "plt.plot(range(1, len(acc)+1), acc, 'rx', label='Train acc')\n",
    "plt.plot(range(1, len(acc)+1), val_acc, 'b', label='Val acc')\n",
    "plt.title('Train & val accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()                 \n",
    "model_simple.save_weights(\"conv.h5\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
